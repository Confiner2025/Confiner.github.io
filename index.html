<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Training-free High-quality Video Generation with Chain of Diffusion Model Experts</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 20px;
            background-color: #f0f0f0;
        }
        header {
            text-align: center;
            margin-bottom: 20px;
        }
        section {
            background: #fff;
            padding: 20px;
            margin-bottom: 20px;
            box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
        }
        h1, h2 {
            color: #333;
        }
        .video-container {
            text-align: center;
            margin: 20px 0;
        }
    </style>
</head>
<body>
    <header>
        <h1>Training-free High-quality Video Generation with Chain of Diffusion Model Experts</h1>
    </header>
    <section>
        <h2>Pipeline</h2>
        <!-- Example image, replace with your actual data visualization -->
        <div class="results">
            <img src="main_fig.pdf" alt="Pipeline" width="1200">
        </div>
    </section>
    <section>
        <h2>Abstract</h2>
        <p>
            Video generation models hold substantial potential in areas such as filmmaking. However, current video diffusion models are still far from practical utility, struggling to balance coherence, clarity, and aesthetics in video generation. In this paper, we propose \textbf{ConFiner}, a training-free framework that decouples the video generation into structural \textbf{con}trol and spatial-temporal re\textbf{fine}ment. It can generate high-quality videos with chain of off-the-shelf diffusion model experts, each expert responsible for their respective tasks. During the refinement, we introduce coordinated denoising, which can utilize multiple diffusion experts within a single timestep. Furthermore, we introduce chain optimizer, which optimizes this framework by modeling the correlations between components. Experimental results indicate that with just 10% of the inference cost, our method surpasses Lavie and Modelscope across all objective and subjective metrics. Specifically, Aesthetic Quality, Coherence, and Visual quality have improved from their previous peaks of 0.597, 0.43, and 0.21 to 0.699, 0.51, and 0.51, respectively.
        </p>
    </section>
    <section>
        <h2>Example Video</h2>
        <div class="video-container">
            <video width="512" controls>
                <source src="mecha.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
    </section>
        <section>
        <h2>Example Video</h2>
        <div class="video-container">
            <video width="512" controls>
                <source src="car.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
    </section>
        <section>
        <h2>Example Video</h2>
        <div class="video-container">
            <video width="512" controls>
                <source src="panda.mp4" type="video/mp4">
                Your browser does not support the video tag.
            </video>
        </div>
    </section>
    <section>
        <h2>Experimental Results</h2>
        <p>
            Here you can showcase the results of your experiments. Include charts, graphs, or text summaries that highlight the key findings of your research.
        </p>
        <!-- Example image, replace with your actual data visualization -->
        <div class="results">
            <img src="experiments.jpg" alt="Experimental Results" width="1200">
        </div>
    </section>
</body>
</html>
